---
title: "Modeling PISA Data by Clusters"
output:
  learnr::tutorial:
    progressive: true
    allow_skip: true

runtime: shiny_prerendered
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dbscan) #computing density based clustering
library(fpc) #computing density based clustering
library(ggrepel) #geom labels
library(kableExtra) #Table layout
library(knitr) #Markdown
library(lme4) #Mixed effects models
library(learnr)
library(maps) #Maps
library(MuMIn) #Mixed effects variance
library(psych) #PCA
library(shiny) #*everything*
library(tidyverse) #data manipulation 
library(DT) #data table formatting
library(plotly)

set.seed(123) #reproducible simulation
```

```{r Exmaple data loading, include=FALSE}
dir <- rstudioapi::getActiveDocumentContext()$path
setwd(dirname(dir))
rm(dir)
```

## Introduction

### **Finding a Question**

You have now learned about enough statistical techniques to try applying them to the full PISA data set. The question that we are going to be interested in is whether policy changes lead to improved scores on the PISA tests when controlling for educational culture. For example, Northern Europe may have better-performing students because it spends more money on education, or it could be because the students experience less financial instability.

The way that we are going to answer this question is by seeing whether countries form clusters with each other. If there are clusters of countries with similar educational environments, we can use mixed-effects models to asses the impact that the policy levers most available to governments, level of spending and the time the students are expected to dedicate to learning, can have. 

The PISA data has thousands of potentially predictive variables. Not only would this lead to the clustering methods taking a long time to run, but correlations between variables can cause problems. We will therefore want to use the PCA techniques to compress the data into more easily handled dimensions.

<p>&nbsp;</p>

### **Loading the data**

The first step in any analysis is loading the data. For the purposes of this tutorial, this will be loading a pre-processed dataset which contains the country-level means for a selection of potentially meaningful variables in the student and school questionnaires.

```{r Heading the final data - ui, eval=TRUE, tidy=TRUE}
countrySummary <- read_csv("countrySummary_incl_Spend.csv")
names(countrySummary[,1:26]) # We will remove the first two variables, turning the Country column into the row names
countrySummary = countrySummary %>%
  column_to_rownames(var = "Country") %>%
  select(-c("CNT"))
```

Looking through the selected variables, it's obvious that many of the variables are measuring similar things and will probably be highly correlated. For example, it makes sense that higher spending on education will lead to fewer staff and material shortages, leading to a lower student-teacher ratio. This is a problem because one of the main weaknesses of clustering algorithms is that they default to equally weighting variables when calculating Euclidean distances. This is why in the previous example the predictive variables were scaled. In this case, leaving the variables as they are now would probably lead to levels of spending being more important than any other relevant factor. The solution to this problem is from the Principal Components Analysis, which will be implemented in the next section.

<p>&nbsp;</p>

## Principal Components Analysis

### **Using PCA**

As we already mentioned, the way around the problem of correlated variables and subsequently uneven weighting is to use the Principal Components Analysis (PCA). There is no definitive answer about how many principal components to choose. 

For this model, we will select the number of components using Kaiser's rule while also aiming to expain ~ 90% of the total variance with our components. This should allow us capture a good amount of the differences between countries while still working towards our aim of data compression.

Note that `prcomp()` will not run if there is missing data. Therefore, we will have to deal with the NA values. For the sake of this exercise, we will remove any country with missing values as we are trying to best answer a question relating to variables rather than regarding specific countries. Other data-cleansing approaches can be applied to deal with this proble, but we will stick with this solution here using `na.omit()`.

```{r Running PCA, eval=TRUE}
cnt_pca <- prcomp(na.omit(countrySummary))

```

```{r Showing PCA, eval=TRUE, tidy=TRUE}
cnt_pca_variance = data.frame(variance = cnt_pca$sdev^2)
variance_matrix = data.matrix(mutate(cnt_pca_variance,cum_variance = cumsum(variance),perc_explained = cum_variance/sum(variance)))


Variance_matrix_components = data.frame(variance = cnt_pca$sdev^2) %>%
  mutate(.,cum_variance = cumsum(variance),perc_explained = cum_variance/sum(variance)) %>%
  datatable(.) %>%
  formatStyle(columns = 'perc_explained',target = 'row', color = styleRow(rows = c(7,11),values = c('darkred','red')))
Variance_matrix_components
```
<p>&nbsp;</p>
### **Choosing number of PCA components**
Based on Kaiser's rule, we see that we should choose no more than 7 components, while if we consider our target of 90% variance explained, we would take 11 components. For the sake of our project, we will use 11 components, as a higher variance explanation is quite significant for us, and 11 components still greatly simplify the original number of variables (halving the number of variables).

```{r}
cnt_pca$rotation[,1:11]
#We can investigate each PC by using this formula : sort(abs(cnt_pca$rotation[,i]),decreasing = TRUE), where i represents the PC that we are interested in
principal_dat = as.data.frame(cnt_pca$x[,1:11]) # Creating a variable with the coordinates of countries onto the PCs to work with later

```

### **Interpreting the Output of PCA**

Looking at the loadings in the correlation matrix, we can see a nice demonstration of what we were trying to avoid as groups of variables are clearly correlated with each other. The variables representing teacher engagement load most strongly onto the first factor. Similarly, the second factor represents teacher engagement and student cooperation, as well as the quality and size of the teacher recruitment pool. Meanwhile,  the third factor seems to be a combination of funding available and student attitude/behavior. 

The interpretation of all the factors is similar, considering the greatest, in absolute value, variable loadings onto a component...


<p>&nbsp;</p>

## Cluster Analysis


### **Density Clustering**
With the variables compressed into principal components, we can move onto the clustering solution. We will use both k-means and density-based clustering, as explained in the clustering section.

The first step is to determine the parameters for the density clustering. As the number of data points is still quite small, MinPts can be set to 3. We can then use a K-Nearest Neighbors plot to find $\epsilon$ by looking for an elbow.

```{r, eval=TRUE}
dbscan::kNNdistplot(principal_dat, k = 3)
abline(h = 4.45, lty = 2)
```

When looking at this graph, the most first obvious elbow is when the distance is 4.45, so this will be used for the analysis.

```{r, eval=TRUE}
db <- fpc::dbscan(cnt_pca$x[,1:11], eps = 4.45, MinPts = 3)
principal_dat$cluster <- as.factor(db$cluster)

db
```

As you can see from this output, the `dbscan` function has determined that there is a single cluster of countries with three outliers. These countries are listed in the table below. This is not ideal for our research plan, which assumed that clusters of countries would be found. It will therefore be interesting to see if this result replicates when using the K-means algorithm.

```{r, echo=FALSE, eval=TRUE}
principal_dat %>%
  filter(cluster == 0) %>%
  select("Cluster" = cluster) %>%
  kable %>% kable_styling(full_width = F, position = "left")

```


### **K-Means Clustering**
The scoring function for the K-Means will be the Calinski-Harabasz index which minimises the ratio between the summed squared distance within the individual clusters and the cluster space as a whole. This tends to lead to evenly shaped and equally sized clusters which can explain less variance but are less vulnerable to overfitting.

```{r, eval=TRUE}

principal_dat = select(principal_dat,-c("cluster"))
km_cluster <- kmeansCBI(principal_dat,
                     krange = 1:(nrow(principal_dat)-1), #Automatically tests every value of K up from one to one less than the number of countries
                     criterion = "ch",
                     runs = 400) #The number of random centroid positions to try

km_cluster$result
```

Here we can see that the countries were split into 2 clusters. One of size 34, and the other of size 22. Furthermore, when looking at the cluster means, we can see very clearly that the biggest difference between the two clusters lays in PC1. To see this visually, we will plot a 2D plot of PC1 and PC2 (as we cant plot all 11 PCs...), where it is quite evident that PC1 is the driving factor in determining to which cluster a country belongs to.

```{r, eval=TRUE}
principal_dat$cluster = as.factor(km_cluster$partition)

ggplotly(
  ggplot(principal_dat, aes(PC1,PC2, color = cluster )) +
    geom_point() +
    theme_minimal(),
  tooltip = "group")
```


<p>&nbsp;</p>

### **Interpreting the Results**

Our two utilized methods provided us with two different outcomes. Density clustering gave us one cluster, with 3 outliers, while K-means gave us two cluster of similar size. In order to decide between the two, it could be a good idea to carry out heirichal clustering and see whether that method shows that there is a noticeable difference between 1 or 2 clusters. However, given that K-means has showed us that the clusters are separated nearly solely by PC1 (which explains only 29% of total variance), we can conclude that using a single cluster would be appropriate. 


## Running the Modified Mixed Effects Model
### **Defining the Model**
Now that we have found that the countries form a single, large cluster, we can move onto modeling the effect of spending and learning time on students' scores. To do this, we will be using a mixed-effects model. Once again, the first step is to load in the data. This analysis will require us to split our dataset into training and testing data, in order to carry out cross validation of our model. We also note that the data is structured in long format, so each row represents one question answered by a student.

```{r,echo = FALSE}
countrySummary <- read_csv("countrySummary_incl_Spend.csv")

dat <- countrySummary %>%
  inner_join(select(read_csv("studentsdata_2018.csv"),c('CNT',"CNTSTUID","PV_Item","Score")), by = "CNT") %>%
  select(c("CNT","GNI","Learning" = "LearningTime","Spending","ID" = "CNTSTUID","PV_Item","Score"))

# We need to split this into training vs testing data

## Defining the proportion of the data to be used for training (the rest will be used for testing)
training_proportion <- 0.2

## Randomly selecting rows from the full dataset (must be in wide format)
training_rows <- sample(1:nrow(dat), floor(nrow(dat)*training_proportion))

training_data <- dat[training_rows, ]

## Selecting the remaining (non-training) rows to be used for testing
testing_data <- dat[-training_rows, ]
rm(training_proportion)

```

<!-- Fix writing here -->
We can now run a model for each type of question answered by the students. In this most basic model, when setting up the random effects, we have the spending and learning time variables which vary by country, each question answered, and the individual taking the test.


```{r}
GLMM_general <- lmer(Score ~ Spending * Learning +
                       (1 | CNT) +
                       (1 | PV_Item),
                     data = training_data)

train_R_squared <- r.squaredGLMM(GLMM_general)[2]
train_margR_squared <- r.squaredGLMM(GLMM_general)[1]
# R^2 = 0.217
# R^2 marginal (variance explained only by fixed effects) = 0.0504

GLMM_general_summary <- summary(GLMM_general)
```

```{r}

# Danai, feel free to try higher complexity models here, but running time might be insane

```
<!-- Need to add comments -->



<!-- NEEDS editing from here-->

### **Results**

Now that we have successfully run the model, we can interpret the results to see if we can answer our original question: Is it always effective to increase education spending and learning time to improve 15-year-olds abilities in reading, mathematics, and science?
```{r}
lme4Summaries <- tibble(
  Subject = c("Reading", "Maths", "Science"),
  Spending = c(GLMM_read_summary$coefficients[2,1], GLMM_math_summary$coefficients[2,1], GLMM_scie_summary$coefficients[2,1]),
  "Learning Time" = c(GLMM_read_summary$coefficients[3,1], GLMM_math_summary$coefficients[3,1], GLMM_scie_summary$coefficients[3,1]),
  "Spending x Learning" = c(GLMM_read_summary$coefficients[4,1], GLMM_math_summary$coefficients[4,1], GLMM_scie_summary$coefficients[4,1]),
  "Marginal R2" = c(r.squaredGLMM(GLMM_read)[1], r.squaredGLMM(GLMM_math)[1],r.squaredGLMM(GLMM_scie)[1]),
  "Total R2" = c(r.squaredGLMM(GLMM_read)[2], r.squaredGLMM(GLMM_math)[2],r.squaredGLMM(GLMM_scie)[2])) %>%
  mutate(across(where(is.numeric), round, 2))

```
```{r, echo=FALSE, eval=TRUE}
kable(lme4Summaries, align = 'c') %>%
  kable_styling
```

The tables above show the fixed effects coefficients, the marginal R^2^, and total R^2^ for each of the models. The marginal R^2^ represents the variance explained by our fixed effects, while total R^2^ represents the variance explained by the whole model. We can see here that spending has the largest impact on student performance, while learning time seems to have a small negative effect. However, it is important to note that the positive interaction between spending and learning time means that more time spent learning will generally lead to better performance if their country's spending is above the mean. At the same time, the fixed effects still only account for ~15% of the variance, which would suggest that there are still many other factors affecting students' performance that are not simply correlated with these two variables.



### **Using the Model to Predict Changes** ####

Below there is a density plot where you can see the distribution of scores and how they relate to changes in spending and learning time. The line of best fit uses the fixed effects from the models we have generated.

```{r Description Graph GUI, eval=TRUE, echo=FALSE}
inputPanel(
  selectInput("x_axis", label = "Key Variable",
              choices = c("Spending", "Learning"), selected = 1),
  selectInput("Type", label = "Question Type",
              choices = c("Reading", "Science", "Maths"), selected = 1)
)
plotOutput("countryPlot")
```

```{r Description Graph server, eval=TRUE, context = 'server', echo=TRUE}
observeEvent(
  {input$x_axis
    input$Type
  },{
    setShow <- switch(input$Type,
                      "Maths" = "MATH",
                      "Reading" = "READ",
                      "Science" = "SCIE")
    
    student_dat <- filter(dat, str_detect(Item, setShow))
    country_dat <- filter(learnSpend, Type == setShow)
    
    intercept <- switch(input$Type,
                        "Maths" = GLMM_math_summary$coefficients[1,1],
                        "Reading" = GLMM_read_summary$coefficients[1,1],
                        "Science" = GLMM_scie_summary$coefficients[1,1]
    )
    if (input$x_axis == "Spending") {
      gradient <- switch(input$Type,
                         "Maths" = GLMM_math_summary$coefficients[2,1],
                         "Reading" = GLMM_read_summary$coefficients[2,1],
                         "Science" = GLMM_scie_summary$coefficients[2,1])
    } else {
      gradient <- switch(input$Type,
                         "Maths" = GLMM_math_summary$coefficients[3,1],
                         "Reading" = GLMM_read_summary$coefficients[3,1],
                         "Science" = GLMM_scie_summary$coefficients[3,1])
    }
    
    output$countryPlot <- renderPlot({
      ggplot(country_dat, aes(x = !!as.name(input$x_axis), y = Mean)) +
        geom_bin2d(data = student_dat, aes(y = Score), bins = 50) +
        scale_fill_gradientn(colours=rainbow(3)) +
        geom_point() +
                geom_abline(slope = gradient, intercept = intercept) +
        geom_text_repel(aes(label = Country), alpha = 1, colour = 'black', size = 3) +
        labs(y = "Score",
             fill = "Count")
    })
  })
```

<!-- NEEDS editing till here -->

<!-- I wont be working on editting PCA/Clustering cross validation, will be for mixed effects model, using the testing dataset -->
## Cross-validation
### **PCA**

The final step in this process is cross-validation. As explained in the previous section, this involves testing our methods on a different dataset to the one which the models were made with. This is often done using a sub-sample of the training data, but we have the advantage of using data from a long-running project. As such, we will validate our findings on the data collected by PISA in 2015. To do this we follow all of the same steps and see whether the same findings emerge. Not all of the same data was collected in the 2015 batch, but the method we have chosen should be robust enough to have the same trends.

```{r, eval=FALSE}
#Read the data
countrySummary2015 <- read_csv("CountrySummary2015.csv")
```
```{r, eval=TRUE, tidy=TRUE}
#PCA
cnt_pca2015 <- principal(countrySummary2015[3:24], nfactors = 11)
cnt_pca2015
```

Looking at this PCA output, 11 principal components are sufficient to reach our threshold of explaining 90% of the variance. While the ordering is slightly different, the variables which load together look to be the same as before. It is therefore suitable to move onto the clustering.

### **Clustering**

For the clustering we will use the same K-Means method as in the original analysis.

```{r, eval=FALSE}
principal_dat2015 <- cbind(countrySummary2015[,2], cnt_pca2015$scores) %>% na.omit
km_cluster2015 <- kmeansCBI(principal_dat2015[2:12],
                            krange = 1:(nrow(principal_dat2015)-1),
                            criterion = "ch",
                            runs = 400)
```

```{r, eval=TRUE, echo=FALSE}
km_cluster2015
```

The results from this clustering also find that a single cluster is the most effective solution, once again agreeing with our original analysis. While a disappointing result, it suggests that our methodological choices are sound.

<!-- New addition by ali, still a work in progress -->
### **Modified Mixed Effects Model predictions**

Earlier, we fit three mixed effects models: GLMM_read, GLMM_math, GLMM_scie. Now we will use the data from 2015 to validate the fit of the models.

```{r}

# Need to remove any rows with missing Learning/Spending values
sum(is.na(testing_data$Learning)) # 133,345 telling us there are NA values
unique(testing_data[is.na(testing_data$Learning),'CNT']) # Only one country, MKD has missing Learning
sum(is.na(testing_data$Spending)) # 0 no NA values...

testing_data_updated <- testing_data[as.logical(1 - is.na(testing_data$Learning)),] %>%
  select(-c('GNI'))

library(lme4)
# Using the model, we predict using testing data and see how it performs
prediction <- predict(GLMM_general, testing_data_updated)

# Subtracting predicted values of y from actual values of y
residuals <- testing_data_updated$Score - prediction 

# Calculating R-Squared
residuals_ss <- sum((residuals)^2)
total_ss <- sum((testing_data_updated$Score - mean(testing_data_updated$Score))^2)

test_R_Squared <- 1 - residuals_ss/total_ss
# R^2 = 0.1952855, very close to the training R^2

rm(residuals)
rm(residuals_ss)
rm(total_ss)
```

<!-- how the data has become set up, we can't do it here, maybe some adjustments will need to be made to accommodate this complex model thing... -->
## Complex Model

A unique quality of the PISA data set is that there is data available at the individual, school, and country level. This is a perfect fit for the advantages of Mixed Effects Models, all-be-it with much-increased computation times. For this reason, as a final complex example, below is an example of a model using this more precise data.

```{r, echo=TRUE, eval=FALSE}

# Create dataset with all variables to be able to run this
dat_2 <- countrySummary %>%
  inner_join(select(read_csv("studentsdata_2018.csv"),c('CNT',"CNTSTUID","PV_Item","Score")), by = "CNT")

# Do whatever you see is fit here, I wouldn't even be opposed to removing this seciton. I think it could be good though, to show that spending and l earning time are the most significant varialbes when it comes to predictive ability.
mixed_model <- lmer(Score ~ Spending * LearningTime + PhysicalInfrastructure + InternetComputers + PropTeachersQual + ClassSize + ParentEducation + Wealth + TeacherInterest + WellBeing + Resilience + Bullied +
                      (1 + Spending | CNT) +
                      (1 + PhysicalInfrastructure + InternetComputers + PropTeachersQual + ClassSize | SchID) +
                      (1 + ParentEducation + LearningTime + Wealth + TeacherInterest + WellBeing + Resilience + Bullied | ID) +
                      (1 | Item),
                    data = #put some sort of a subset of this big dat_2 dataset
                      )
```

```{r, eval=TRUE}
summary(mixed_model)
r.squaredGLMM(mixed_model)
```

Based on this final output, we can see that educational spending is once again the greatest predictive factor for scores on all questions in the PISA data set. This is useful to know because it means that generally increasing spending on education will lead to tangible improvements in outcomes. Less encouraging is the general lack of impact of cultural predictors, with the next two strongest fixed effects being household wealth and parental education. These are much harder to influence in the short term, though it would suggest that investment in education will continue to have an impact over a generation.

The other consideration is that even with more variables in finer detail, the model was only able to account for ~30% of the variance. This would suggest that there are still more factors which are influencing students' scores.

<p>&nbsp;</p>
## Final Reflections

As much as we would wish it otherwise, ultimately all statistical methods consist of a series of judgment calls. Each decision branch has pros and cons and it is not immediately obvious what these will be. For example, using the linear mixed-effects model on subsections of the PISA dataset was effective, finding robust results despite the lower power. However, the processing power needed to construct these models is prohibitive and essentially rules out using the unique quantity of data available in PISA. At the same time, it may also be necessary to use the data compression in PCA and clustering to make the job of interpretation easier for the humans at the end of the process.

Ultimately, our efforts in this learning app are a step forward in applying more sophisticated analyses to the PISA set, rather than relying on simple comparisons of country-level means. Potential future directions could either build on what we have achieved here or start from a completely different point. For example, K-means is not necessarily the best clustering method in this situation, how would the results change if density clustering was used? Another advancement would be more effectively using the breadth of data available, such as using mixed-effect models which vary by student, school, and teacher.

### Final Review Questions

Congratulations for making it all the way to the end of our tutorial! We hope that you have found it useful. As a final test of your understanding of the concepts that we have explored, we have included the following questions.

```{r Model_Quiz, echo=FALSE}
quiz(
  question("Why was PCA used in this analysis?",
           answer("Accounting for correlations between variables", correct = TRUE),
           answer("Making it easier to keep track of predictors"),
           answer("Finding meaningful factors in the data"),
           answer("To make reviewers think we know what we're talking about"),
           allow_retry = TRUE 
  ),
  question("What are potential drawbacks of modeling a large heterogenous dataset?",
           answer("Difficult to validate"),
           answer("Is computationally expensive", correct = TRUE),
           answer("Lower power"),
           allow_retry = TRUE 
  ),
  question("Using a mixed-effects model...",
           answer("Improves interpretability"),
           answer("Accounts for more unsystematic variance", correct = TRUE),
           answer("Is less computationally taxing"),
           allow_retry = TRUE 
  ),
  question("Where possible, why is it a good idea to use cross-validation?",
           answer("Find the data which best fits the model"),
           answer("Estimate the true population parameter"),
           answer("Ensures that the model is stable and consistent", correct = TRUE),
           allow_retry = TRUE)
)
```
