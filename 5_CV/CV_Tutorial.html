<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />

<meta name="author" content="David Aikman" />


<meta name="progressive" content="true" />
<meta name="allow-skip" content="true" />

<title>Principle Component Analysis</title>


<!-- highlightjs -->
<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>

<!-- taken from https://github.com/rstudio/rmarkdown/blob/67b7f5fc779e4cfdfd0f021d3d7745b6b6e17149/inst/rmd/h/default.html#L296-L362 -->
<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("section-TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>
<!-- end tabsets -->



</head>

<body>



<div class="pageContent band">
<div class="bandContent page">

<div class="topics">

<pre><code>To do list
* add references

* add quiz

* add section on &#39;external&#39; cross validation

* Fix js 

* add practical PISA example

* format for tutorial set-up</code></pre>
<hr>
<script src="https://cdnjs.cloudflare.com/ajax/libs/p5.js/1.1.9/p5.js"></script>
<!-- this is some javascript to make the interactive K-means thing -->
<script type="text/javascript">
let n = 36;
let p = 16;
let gridSize = 16;
let pointSize = 12;
let k = 12;

let kVec = [2, 4, 8, 16, 32];

let nSlider, pSlider, pause;

let stg = 0;
let stgMove = 0;
let phase = 0;

let rands = [];
let points = [];
let pointsShuffled = [];

function setup() {
  kFoldsDiv = createCanvas(400, 200);
  kFoldsDiv.parent('kFoldsDiv');
  
  nSlider = createSlider(32, 192, 144, 32);
  nSlider.position(0, 180);
  nSlider.parent('kFoldsDiv')
  nSlider.style('width', '128px')
  nSlider.input(reset);
  
  kSlider = createSlider(0, 4, 1, 1);
  kSlider.position(140, 180);
  kSlider.parent('kFoldsDiv')
  kSlider.style('width', '128px')
  kSlider.input(reset);
  
  for (let i = 0; i < n; i++) {
    rands[i] = i;
  }
  
  k = kVec[kSlider.value()];
  stgMove = floor(n/k);
  
  n = nSlider.value();
  p = stgMove;
  
  for (let i = 0; i < n; i++) {
    
    points[i] = new Point(gridSize+i%24*gridSize, gridSize+floor(i/24)*gridSize, pointSize, 
                          i, i, points);
      
  }
  
  pointsShuffled = shuffle(points, false);
  
}

function draw() {
  background(255);
  if (phase == 0) {
    phase0();
  } else if (phase == 1) {
    phase1();
  } else if (phase == 2) {
    //points = [];
    phase2();
  }
  
}

function phase1() {
  
  // checking two points have finished moving
  if (round(points[0].x) == pointsShuffled[points[0].i1].xStart & frameCount % 40 == 0) {
    if (round(points[10].x) == pointsShuffled[points[10].i1].xStart & frameCount % 40 == 0) {
      phase = 2;
    } 
  }

  for (let i = 0; i < n; i++) {
    
    points[i].move(); 
    points[i].show1();
    
    if (i % p == 0) {
      stroke(255, 50, 50);
      strokeWeight(2)
      line(gridSize+i%24*gridSize-gridSize/2, gridSize+floor(i/24)*gridSize+6, 
           gridSize+i%24*gridSize-gridSize/2, gridSize+floor(i/24)*gridSize-6)
    }
      
  }
  
  
  
  n = nSlider.value();
  k = kVec[kSlider.value()];
  
  // TEXT
  textSize(24);
  fill(0);
  text("N = ", 8, 180);
  text(n, 52, 180);
  
  text("k = ", 148, 180);
  text(k, 188, 180);
  
  // Shuffling!
  text("Shuffling...", 278, 194)

}

function phase0() {
  k = kVec[kSlider.value()];
  stgMove = floor(n/k);
  
  n = nSlider.value();
  p = stgMove;
  
  
  for (i = 0; i < n; i++) {
    fill(50, 50, 255);
    noStroke();
    ellipse(gridSize+i%24*gridSize, gridSize+floor(i/24)*gridSize, pointSize)
    if (i % p == 0) {
      stroke(255, 50, 50);
      strokeWeight(2)
      line(gridSize+i%24*gridSize-gridSize/2, gridSize+floor(i/24)*gridSize+6, 
           gridSize+i%24*gridSize-gridSize/2, gridSize+floor(i/24)*gridSize-6)
    }
  }
  
  // TEXT
  textSize(24);
  fill(0);
  text("N = ", 8, 180);
  text(n, 52, 180);
  
  text("k = ", 148, 180);
  text(k, 188, 180);
  
  if (p > n) {
    fill(255, 50, 50);
    textSize(16);
    text("p > N", 236, 176);
  }
  
  // BUTTON
    fill(50, 50, 240);
    noStroke(0);
    rect(288, 160, 96, 36, 20);
    fill(255);
    textSize(24);
    text("Start", 308, 186);
  
}

function mousePressed() {
  
  if (mouseX > 288 & mouseX < 288+96 & mouseY > 160 & mouseY < 160+36 & phase == 0) {
    phase = 1;
    for (let i = 0; i < n; i++) {
      points[i] = new Point(gridSize+i%24*gridSize, gridSize+floor(i/24)*gridSize, pointSize, 
                            i, i, points);
    }
  
  pointsShuffled = shuffle(points, false);
  }
 
}

function phase2() {
  
  k = kVec[kSlider.value()];
  stgMove = floor(n/k);
  
  if (frameCount % 40 == 0 & stg < n) {
    stg += stgMove;
  }
  
  if (stg > n) {
    stg = 0;
  }
  
  n = nSlider.value();
  p = stgMove;
  
  startP = stg;
  endP = startP + p;
  
  for (i = 0; i < n; i++) {
    if (endP <= n) {
      if (i+1 <= endP & i+1 > startP & stg < n) {
        fill(255, 50, 50);
      } else {
        fill(50, 50, 255);
      }
    } else {
    
      if (i+1 > startP | i+1 <= endP-n & stg < n) {
        fill(255, 50, 50);
      } else {
        fill(50, 50, 255);
      }
    }
    
    noStroke();
    ellipse(gridSize+i%24*gridSize, gridSize+floor(i/24)*gridSize, pointSize)
    
  }
  
  
  // TEXT
  textSize(24);
  fill(0);
  text("N = ", 8, 180);
  text(n, 52, 180);
  
  text("k = ", 148, 180);
  text(k, 188, 180);
  
  if (p > n) {
    fill(255, 50, 50);
    textSize(16);
    text("p > N", 236, 176);
  }
  
  // KEY
  noFill();
  stroke(0);
  rect(288, 144, 96, 48);
  noStroke();
  fill(50, 50, 255);
  ellipse(300, 158, pointSize);
  fill(0);
  textSize(16);
  text("= training", 312, 164);
  
  fill(255, 50, 50);
  ellipse(300, 178, pointSize);
  fill(0);
  textSize(16);
  text("= testing", 312, 184);
  
  if (stg >= n) {
    
    phase = 0;
    stg = 0;
  
  }
  
}

function togglePause() {
  if (stgMove == 1) {
    stgMove = 0;
  } else {
    stgMove = 1;
  }
}

function reset() {
  phase = 0;
  stg = 0;
  stgMove = 0;
  
  for (let i = 0; i < n; i++) {
    rands[i] = i;
  }
  
  points = [];
  k = kVec[kSlider.value()];
  stgMove = floor(n/k);
  
  n = nSlider.value();
  p = stgMove;
  
  for (let i = 0; i < n; i++) {
    
    points[i] = new Point(gridSize+i%24*gridSize, gridSize+floor(i/24)*gridSize, pointSize, 
                          i, i);
      
  }
  
  pointsShuffled = shuffle(points, false);
  
}

class Point {
  constructor(x, y, size, index_2, index_1, vector) {
  
      this.x = x;
      this.y = y;
      this.xStart = x;
      this.yStart = y;
      this.size = size;
      this.i1 = index_1;
      this.i2 = index_2;
      
      this.vector = vector;
  
  }
  
  move() {
  
    this.x = lerp(this.x, pointsShuffled[this.i1].xStart, 0.05);
    this.y = lerp(this.y, pointsShuffled[this.i1].yStart, 0.05);
  
  }
  
  show1() {
    
    noStroke();
    fill(50, 50, 255)
    ellipse(this.x, this.y, this.size)
  
  }
  
  show2() {
  
    if (endP <= n) {
      if (this.i2+1 <= endP & this.i2+1 > startP) {
        fill(255, 50, 50);
      } else {
        fill(50, 50, 255);
      }
    } else {
    
      if (this.i2+1 > startP | this.i2+1 <= endP-n) {
        fill(255, 50, 50);
      } else {
        fill(50, 50, 255);
      }
    }
    
    noStroke();
    ellipse(this.x, this.y, this.size)
  
  }
  
}


</script>
<div id="section-what-is-cross-validation" class="section level2">
<h2>What is Cross-Validation?</h2>
<div id="section-introduction" class="section level3">
<h3>Introduction</h3>
<p>After fitting a model to your data, you might want to know how well it would fit other, similar data. If your model does not fit other data very well, it may not be representative of the true relationships between the variables in your model. It may only represent relationships in the data used to fit the model. Determining the fit of a model on data that was not used to derive model parameters is known as cross-validation.</p>
<p>In some cases, you would be able to collect new data to perform cross-validation, i.e., <em>observe</em> the performance of your model on newly collected data. In others, it may be unfeasible to collect more data. In this case, you can <em>estimate</em> the performance of your model on new data with your original data. This tutorial is to illustrate the methods to perform cross-validation on your original data.</p>
</div>
<div id="section-holdout-cross-validation" class="section level3">
<h3>Holdout Cross-Validation</h3>
<p>Cross-validation methods that do not require new data collection are based on the idea of a <em>holdout</em> sample. You split your data into two parts: a training set and a testing (or <em>holdout</em>) set. A model is fitted on the training set and ‘tested’ on the holdout set, and the performance of your model, estimated with the training set, on the testing set is your estimation of your model’s generalizability on unseen data. The idea of ‘testing’ here refers to checking how well the model fits to the holdout sample. If the model fits well on both samples, this offers evidence for the generalizability of your model to a hypothetical larger sample of data (REF). The estimation error of the model in the testing set (usually MSE) is termed testing error. We can also use other indices (e.g., R<sup>2</sup>, AIC, BIC) to measure test error.</p>
<p><em>DISCUSSION ABOUT PROPORTIONS</em> To estimate model performance accurately, both the training set &amp; the testing set need to be representative of the original dataset. If the training or testing data are unrepresentative of the complete data then the results from the test sets can be biased. As we all know, the goodness of a model is also determined by the sample size of the dataset used to estimate it. The smaller the sample size, the worse the model. Therefore, we tend to make the training set as large as possible. Typically, a training set is over half of the original sample (say, 80%).</p>
<p>However, as the training set is always smaller than the original dataset, the performance of the model estimated with the training set tends to be worse than the model estimated with the original dataset. That is, the holdout CV tend to overestimate test errors and underestimate model fit.</p>
<div id="section-holdoutDiv" style="position: relative;">

</div>
</div>
<div id="section-leave-one-out-cross-validation-loocv" class="section level3">
<h3>Leave one out cross-validation (LOOCV)</h3>
<p>LOOCV is a extension of the holdout method. Like the holdout method, LOOCV involves splitting the set of observations into two parts. However, instead of creating two subsets of comparable size, a single data point <span class="math inline">\((x_1, y_1)\)</span> is the holdout set, and the remaining observations <span class="math inline">\(\{(x_2, y_2 ), ... , (x_n, y_n)\}\)</span> compose the training set. The model is ﬁt on the <span class="math inline">\(n − 1\)</span> training observations, and a prediction <span class="math inline">\(\hat{y}_1\)</span> is made for the excluded observation, using its value <span class="math inline">\(x_1\)</span>. The estimation of test error with <span class="math inline">\((x_1, y_1)\)</span> would be MSE<sub>1</sub> = <span class="math inline">\((y_1 - \hat{y}_1)^2\)</span>. As we only have used a single observation, this estimation of test error is biased. To obtain an unbiased estimation of test error, we can repeat the procedure until we loop through all observations in the original dataset, and then average all the estimated test errors. The equation for the estimation of test errors with LOOCV would be:</p>
<p><span class="math display">\[\mathrm{CV_{(n)}} = \frac{1}{n}\sum_{i = 1}^n \mathrm{MSE_i}\]</span> where n is the sample size of the original dataset, and i is the ith data point, and MSE<sub>i</sub> = <span class="math inline">\((y_i - \hat{y}_i)^2\)</span></p>
<p><em>LOOCV is better than the holdout method in many ways</em></p>
<ul>
<li><p>First, it is far less biased. The results of holdout CV is highly subjective to the division of training and testing set. In contrast, LOOCV always yields the same result. LOOCV loops through all observations in the dataset and uses the mean MSE, which is an unbised estimation of test error, as the result.</p></li>
<li><p>Second, as LOOCV uses a much larger training set than holdout CV, it tends not to overestimate test error as much as holdout CV does.</p></li>
</ul>
<p>However, LOOCV is computational heavy, especially when sample size is large. Therefore, we often use k-fold cross-validation method, which also generates unbiased estimation for test errors but requires far less computation.</p>
</div>
<div id="section-k-fold-cross-validation" class="section level3">
<h3>k-fold cross-validation</h3>
<p>Like LOOCV, k-fold CV is also an iterated version of the simple holdout algorithm, You randomly partition your dataset into <span class="math inline">\(k\)</span> subsamples (commonly k = 5 or 10). One after another, each sub-sample is used as the testing set for a model fit to the data in all other sub-samples. Metrics of interest are averaged across iterations, the result of which can be used to draw conclusions about the generalisability of the model (REF). By iterating over cross-validation algorithms, the bias of model-fit indicators (e.g., RMSEA, BIC) is also reduced compared to simple, singular holdout cross-validation (REF). for the estimation of test errors with k-fold would be:</p>
<p><span class="math display">\[\mathrm{CV_{(k)}} = \frac{1}{k}\sum_{i = 1}^k \mathrm{MSE_k}\]</span></p>
<!-- The **Leave-p-Out** algorithm randomly selects $p$ observations to exclude from the training set. These $p$ observations constitute the testing set. A model is fitted to the training data and tested on the testing data. The selection, training and testing process is repeated until all possible combinations of $p$ observations are used as a testing set. The number $p$ could theoretically be any number smaller than the number of observations in the full dataset ($N$). For some values of $p$ there will be many, *many* possible configurations of training and testing data. If $N = 1000$ and $p = 50$, then the total number of ways to split the data into testing and training is around $10\times e^{84}$. (*Note: you can calculate this for other combinations of $N$ and $p$ using the R code* `choose(N, p)`.) -->
<p>See below for a demonstration of the k-folds algorithm.</p>
<em>THIS DOESN’T WORK WITH SHINY_PRERENDERED - MIGHT WORK WHEN HOSTED ON A SERVER</em> <em>USE THIS LINK TO SEE WHAT IT’S SUPPOSED TO LOOK LIKE <a href="https://editor.p5js.org/daikman/present/U2GomNWcK" class="uri">https://editor.p5js.org/daikman/present/U2GomNWcK</a></em>
<div id="section-kFoldsDiv" style="position: relative; border-style: solid; width: 406px">

</div>
<p><em>k-fold CV has multiple benefits over LOOCV</em></p>
<ul>
<li><p>Computational benefits: iterate k times instead of n times</p></li>
<li><p>The second benefit lies in the bias-variance trade-off.</p></li>
</ul>
<p>As mentioned before, cross-validation tend to overestimate test errors, especially when the training sample size is small. Therefore, holdout CV tends to overestimate the test error the most, and LOOCV gives approximately unbiased estimation as its training sample size is n-1, which is almost the same as the original sample size. k-fold CV is intermediately biased. Therefore, LOOCV has advantage over k-fold CV in terms of bias-reduction.</p>
<p>However, k-fold CV has less variance than LOOCV. As each time only one data point is singled out, the samples used to train the model are highly similar to each other in LOOCV, whereas in k-fold CV, the samples used to train the model are much less similar to each other. Therefore, the effective degrees of freedom of LOOCV is much smallers than k-fold CV. This puts LOOCV at a higher risk of overfitting, i.e., more variance.</p>
<p><em>DISCUSS VALUE OF K</em></p>
<p>The above arguments also indicate the impact of value of k on the results of k-fold CV. The larger the value of k, the more similar k-fold CV is to LOOCV, and the less bias and the more variance. The most commonly used values are k = 5 and 10.</p>
</div>
<div id="section-external-cross-validation" class="section level3">
<h3>External Cross-Validation</h3>
</div>
</div>
<div id="section-cross-validation-in-r" class="section level1">
<h1>Cross-validation in R</h1>
<div id="section-holdout-cross-validation-1" class="section level2">
<h2>Holdout Cross-Validation</h2>
<div id="section-simulating-a-dataset" class="section level3">
<h3>Simulating a Dataset</h3>
<p>The plot below shows a clear positive relationship between variables <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. To demonstrate the application of cross-validation, we will train and test the model <span class="math inline">\(y_{i} = \beta_0 + \beta_1x_{i}\)</span> using this dataset. This is a simple linear regression model.</p>
<p><img src="CV_Tutorial_files/figure-html/unnamed-chunk-2-1.png" width="624" /></p>
</div>
<div id="section-splitting-the-dataset" class="section level3">
<h3>Splitting the Dataset</h3>
<p>This first step for simple holdout cross-validation is splitting the dataset into testing and training data.</p>
<pre class="r"><code># Defining the proportion of the data to be used for training (the rest will be used for testing)
training_proportion &lt;- 0.8

# Randomly selecting rows from the full dataset (must be in wide format)
training_rows &lt;- sample(1:nrow(dataset), floor(nrow(dataset)*training_proportion))

training_data &lt;- dataset[training_rows, ]

# Selecting the remaining (non-training) rows to be used for testing
testing_data &lt;- dataset[-training_rows, ]</code></pre>
</div>
<div id="section-training-the-model" class="section level3">
<h3>Training the Model</h3>
<p>Next, we can fit a model to the training data and find the desired fit index (<span class="math inline">\(R^2\)</span> in this case).</p>
<pre class="r"><code># Fitting a model to the training data
train.fit &lt;- lm(y ~ x, training_data)

# Extracting R-Squared from the training model
train_R_Squared &lt;- summary(train.fit)$r.squared # (it&#39;s around 0.8)</code></pre>
</div>
<div id="section-predicting-values-in-the-test-data" class="section level3">
<h3>Predicting Values in the Test Data</h3>
<p>Using the model fitted to the training data, we need to calculate predictions for the values of <span class="math inline">\(y\)</span> in the testing data based on the observed values of <span class="math inline">\(x\)</span> in the testing data and the parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> from the model fitted above. The <code>predict()</code> function from the <code>stats</code> package can plug in the <span class="math inline">\(x\)</span> values from the testing data into the model in order to estimate the <span class="math inline">\(y\)</span> values.</p>
<pre class="r"><code># Predicting values of y, using the model fit to the training data, from the values of x in the testing data.
prediction &lt;- predict(train.fit, data.frame(x = testing_data$x))</code></pre>
</div>
<div id="section-calculating-test-r2" class="section level3">
<h3>Calculating Test <span class="math inline">\(R^2\)</span></h3>
<p><span class="math inline">\(R^2\)</span> for this prediction can be calculated from the differences between each predicted value of <span class="math inline">\(y\)</span> and the corresponding observed value of <span class="math inline">\(y\)</span>. These differences are known as residuals. The <span class="math inline">\(R^2\)</span> statistic, in this case, will tell us how well the model parameters derived from the training sample fit the data in the testing sample.</p>
<pre class="r"><code># Subtracting predicted values of y from actual values of y
residuals &lt;- testing_data$y - prediction

# Calculating R-Squared
residuals_ss &lt;- sum((residuals)^2)
total_ss &lt;- sum((testing_data$y - mean(testing_data$y))^2)

test_R_Squared &lt;- 1 - residuals_ss/total_ss</code></pre>
<p>The model fitted to the training data fits that data with an <span class="math inline">\(R^2\)</span> of around 0.792. This same model fit the testing data with an <span class="math inline">\(R^2\)</span> of around 0.836.</p>
<p>This means that the model fit both sub-samples of data well. The fact that the testing data was ‘unseen’ during the model fitting process suggests the model would fit well to other unseen data (e.g., data collected in the future), providing evidence of predictive validity.</p>
</div>
</div>
<div id="section-k-folds-cross-validation" class="section level2">
<h2>K-folds Cross-Validation</h2>
<div id="section-introduction-1" class="section level3">
<h3>Introduction</h3>
<p>Implementing <span class="math inline">\(k\)</span>-folds cross validation works much the same as above, only we repeat the process using different divisions of the data (into training and testing sub-samples) each time. We can essentially wrap all the R code above in <a href="https://r4ds.had.co.nz/iteration.html#the-map-functions">a loop, or use an iterative function</a>.</p>
</div>
<div id="section-choosing-k" class="section level3">
<h3>Choosing <span class="math inline">\(k\)</span></h3>
<p>First, we need to decide on the value of <span class="math inline">\(k\)</span>. For this demonstration, we will set <span class="math inline">\(k\)</span> to 5.</p>
<pre class="r"><code>k &lt;- 5</code></pre>
</div>
<div id="section-splitting-the-data" class="section level3">
<h3>Splitting the Data</h3>
<p>Next, we need to split the data into <span class="math inline">\(k\)</span> (5) training and testing sets, where the testing sets are 1/5th the size of the whole dataset.</p>
<pre class="r"><code># Shuffling the data

## generating new random row numbers
set.seed(123)
new_positions &lt;- sample(1:nrow(dataset), nrow(dataset), replace = FALSE)

## sorting dataset by random row numbers
dataset &lt;- arrange(dataset, new_positions)

# Dividing data into k training and testing sets
## pre-allocating list to store results of the loop below
testing_indexes &lt;- list()

## using a loop to define the rows of data to use for testing in each fold
## the mathematical part of this loop produces the numbers 1-400, 401-800... when k = 5.
for (i in 1:k) {
  testing_indexes[[i]] &lt;- floor(
    (1+(i-1)*nrow(dataset)/k):((i)*nrow(dataset)/k)
  ) 
}</code></pre>
</div>
<div id="section-iterating-through-the-samples" class="section level3">
<h3>Iterating Through the Samples</h3>
<p>Now we need to iterate through all the testing sub-samples (as defined by <code>testing_indexes</code>) using the rest of the data to train the model. (This will be the same model as above: <span class="math inline">\(y_{i} = \beta_0 + \beta_1x_{i}\)</span>).</p>
<pre class="r"><code># Pre-allocating vectors to store the results of each iteration
train_R_Squared_Repeated &lt;- vector()
test_R_Squared_Repeated &lt;- vector()

# &#39;Looping through&#39; each of the training/testing data combinations. 
## this loop could be combined with the loop above (as they both use `i in 1:k`).
for (i in 1:k) {
  
  # Selecting the testing data from the full dataset, using the indexes defined above
  testing_data &lt;- dataset[testing_indexes[[i]], ]

  # Selecting the remaining (non-training) rows to be used for testing
  training_data &lt;- dataset[-testing_indexes[[i]], ]
  
  # Fitting a model to the training data
  train.fit &lt;- lm(y ~ x, training_data)
  
  # Extracting R-Squared from the training model
  train_R_Squared_Repeated[i] &lt;- summary(train.fit)$r.squared
  
  # Predicting values of y, using the model fit to the training data, from the values of x in the testing data.
  prediction &lt;- predict(train.fit, data.frame(x = testing_data$x))
  
  # Subtracting predicted values of y from actual values of y
  residuals &lt;- testing_data$y - prediction
  
  # Calculating R-Squared
  residuals_ss &lt;- sum((residuals)^2)
  total_ss &lt;- sum((testing_data$y - mean(testing_data$y))^2)
  
  test_R_Squared_Repeated[i] &lt;- 1 - residuals_ss/total_ss
  
}</code></pre>
<div style="width: 50%;">
<table>
<caption>The R-Squared values for each of the datasets</caption>
<thead>
<tr class="header">
<th align="right">Fold</th>
<th align="right">Testing</th>
<th align="right">Training</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">1</td>
<td align="right">0.8009482</td>
<td align="right">0.8015596</td>
</tr>
<tr class="even">
<td align="right">2</td>
<td align="right">0.8102337</td>
<td align="right">0.7991945</td>
</tr>
<tr class="odd">
<td align="right">3</td>
<td align="right">0.8008074</td>
<td align="right">0.8016142</td>
</tr>
<tr class="even">
<td align="right">4</td>
<td align="right">0.8018985</td>
<td align="right">0.8012767</td>
</tr>
<tr class="odd">
<td align="right">5</td>
<td align="right">0.7898264</td>
<td align="right">0.8040950</td>
</tr>
</tbody>
</table>
</div>
<p><img src="CV_Tutorial_files/figure-html/unnamed-chunk-11-1.png" width="624" /></p>
<p>The table and figure above display the <span class="math inline">\(R^2\)</span> values for each of the sub-samples, for each ‘fold’. The results are similar across folds, with all <span class="math inline">\(R^2\)</span> values being quite high.</p>
<p>In the figure, you can see that models that fit better to the training data fit worse on the testing data. This is known as <em>overfitting</em>, which occurs when a model’s parameters are fit so well to a dataset that they are highly influenced by the random error in the that dataset, making them less accurate for unseen data, as unseen data will not contain the same random error.</p>
<p>However, in this case we are not interested in overfitting. We want to know the results of our repeated cross-validation. To do that, we can average the <span class="math inline">\(R^2\)</span> value (or any other fit index) over iterations.</p>
<p>The mean <span class="math inline">\(R^2\)</span> for the training sub-samples is 0.802 (to 3 decimal places), which is slightly higher than the mean <span class="math inline">\(R^2\)</span> for the testing sub-samples, 0.801.</p>
<p>The conclusion we could draw from this analysis is the same as the conclusion we drew above, from the simple holdout cross-validation. The model performs well on unseen data, providing evidence of its predictive validity.</p>
</div>
</div>
<div id="section-loocv" class="section level2">
<h2>LOOCV</h2>
<pre class="r"><code># &#39;Looping through&#39; all data points 

for (i in 1:nrow(dataset)) {
  
  # Selecting the testing data from the full dataset, using the indexes defined above
  testing_data &lt;- dataset[i, ]

  # Selecting the remaining (non-training) rows to be used for testing
  training_data &lt;- dataset[-i, ]
  
  # Fitting a model to the training data
  train.fit &lt;- lm(y ~ x, training_data)
  
  # Extracting R-Squared from the training model
  train_R_Squared_Repeated[i] &lt;- summary(train.fit)$r.squared
  
  # Predicting values of y, using the model fit to the training data, from the values of x in the testing data.
  prediction &lt;- predict(train.fit, data.frame(x = testing_data$x))
  
  # Subtracting predicted values of y from actual values of y
  residuals[i] &lt;- testing_data$y - prediction
  
}

  
# Calculating R-Squared
residuals_ss &lt;- sum((residuals)^2)/nrow(dataset)
total_ss &lt;- var(dataset$y)

test_R_Squared &lt;- 1 - residuals_ss/total_ss
train_R_Squared &lt;- mean(train_R_Squared_Repeated)

# make a result table
LOOCV &lt;- data.frame(Testing = test_R_Squared, Training = train_R_Squared)

knitr::kable(
  LOOCV, 
  styling = &#39;html&#39;,
  caption = &#39;The R-Squared values for LOOCV&#39;
)</code></pre>
<table>
<caption>The R-Squared values for LOOCV</caption>
<thead>
<tr class="header">
<th align="right">Testing</th>
<th align="right">Training</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.8012688</td>
<td align="right">0.8015495</td>
</tr>
</tbody>
</table>
</div>
<div id="section-external-cv" class="section level2">
<h2>External CV</h2>

<script type="application/shiny-prerendered" data-context="server-start">
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
</script>
 <!--html_preserve-->
<script type="application/shiny-prerendered" data-context="dependencies">
{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.4"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootstrap"]},{"type":"character","attributes":{},"value":["3.3.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/bootstrap"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["viewport"]}},"value":[{"type":"character","attributes":{},"value":["width=device-width, initial-scale=1"]}]},{"type":"character","attributes":{},"value":["js/bootstrap.min.js","shim/html5shiv.min.js","shim/respond.min.js"]},{"type":"character","attributes":{},"value":["css/cerulean.min.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.4"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["pagedtable"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/pagedtable-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/pagedtable.js"]},{"type":"character","attributes":{},"value":["css/pagedtable.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.4"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["textmate.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.4"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.10.1.9006"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1.9006"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.10.1.9006"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1.9006"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.10.1.9006"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1.9006"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-format"]},{"type":"character","attributes":{},"value":["0.10.1.9006"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmarkdown/templates/tutorial/resources"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-format.js"]},{"type":"character","attributes":{},"value":["tutorial-format.css","rstudio-theme.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1.9006"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.4"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["navigation"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/navigation-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tabsets.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.4"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["default.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.4"]}]}]}
</script>
<!--/html_preserve-->
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="execution_dependencies">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages","version"]},"class":{"type":"character","attributes":{},"value":["data.frame"]},"row.names":{"type":"integer","attributes":{},"value":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81]}},"value":[{"type":"character","attributes":{},"value":["assertthat","backports","base","blob","broom","cellranger","cli","colorspace","compiler","crayon","curl","datasets","DBI","dbplyr","digest","dplyr","ellipsis","evaluate","fansi","farver","fastmap","forcats","fs","generics","ggplot2","glue","graphics","grDevices","grid","gtable","haven","highr","hms","htmltools","htmlwidgets","httpuv","httr","jsonlite","knitr","labeling","later","learnr","lifecycle","lubridate","magrittr","markdown","methods","mime","modelr","munsell","pillar","pkgconfig","promises","purrr","R6","Rcpp","readr","readxl","reprex","rlang","rmarkdown","rprojroot","rstudioapi","rvest","scales","shiny","stats","stringi","stringr","tibble","tidyr","tidyselect","tidyverse","tools","utils","vctrs","withr","xfun","xml2","xtable","yaml"]},{"type":"character","attributes":{},"value":["0.2.1","1.1.10","4.0.3","1.2.1","0.7.1","1.1.0","2.0.2","1.4-1","4.0.3","1.3.4","4.3","4.0.3","1.1.0","1.4.4","0.6.25","1.0.2","0.3.1","0.14","0.4.1","2.0.3","1.0.1","0.5.0","1.5.0","0.0.2","3.3.2","1.4.2","4.0.3","4.0.3","4.0.3","0.3.0","2.3.1","0.8","0.5.3","0.5.0","1.5.2","1.5.4","1.4.2","1.7.1","1.30","0.3","1.1.0.1","0.10.1.9006","0.2.0","1.7.9","1.5","1.1","4.0.3","0.9","0.1.8","0.5.0","1.4.6","2.0.3","1.1.1","0.3.4","2.4.1","1.0.5","1.4.0","1.3.1","0.3.0","0.4.7","2.4","1.3-2","0.11","0.3.6","1.1.1","1.5.0","4.0.3","1.5.3","1.4.0","3.0.3","1.1.2","1.1.0","1.3.0","4.0.3","4.0.3","0.3.4","2.3.0","0.18","1.3.2","1.8-4","2.2.1"]}]}]}
</script>
<!--/html_preserve-->
</div>
</div>

</div> <!-- topics -->

<div class="topicsContainer">
<div class="topicsPositioner">
<div class="band">
<div class="bandContent topicsListContainer">

<!-- begin doc-metadata -->
<div id="doc-metadata">
<h2 class="title toc-ignore" style="display:none;">Principle Component Analysis</h2>
<h4 class="author"><em>David Aikman</em></h4>
</div>
<!-- end doc-metadata -->

</div> <!-- bandContent.topicsListContainer -->
</div> <!-- band -->
</div> <!-- topicsPositioner -->
</div> <!-- topicsContainer -->


</div> <!-- bandContent page -->
</div> <!-- pageContent band -->




<script>
// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>


</body>

</html>
